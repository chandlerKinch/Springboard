{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab15ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import CommentPull.CommentPuller\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011ffb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make graphs pretty and bigger\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f44baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data using custom class\n",
    "comment_puller = CommentPull.CommentPuller.CommentPuller(load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f46d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split title data\n",
    "X_title = comment_puller.title_df.drop(['label_id', 'post_id'], axis=1)\n",
    "y_title = comment_puller.title_df['label_id']\n",
    "X_title_train, X_title_test, y_title_train, y_title_test = train_test_split(X_title, y_title,\n",
    "                                                                           test_size=0.15, random_state=10)\n",
    "#Scale training data\n",
    "title_scaler = RobustScaler()\n",
    "X_title_train = title_scaler.fit_transform(X_title_train)\n",
    "\n",
    "#Scale test data in same manner\n",
    "X_title_test = title_scaler.transform(X_title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b2985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split text data \n",
    "X_text = comment_puller.text_df.drop(['label_id', 'post_id'], axis=1)\n",
    "y_text = comment_puller.text_df['label_id']\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text, y_text,\n",
    "                                                                           test_size=0.15, random_state=10)\n",
    "#Scale training data\n",
    "text_scaler = RobustScaler()\n",
    "X_text_train = text_scaler.fit_transform(X_text_train)\n",
    "#Scale test data in same manner as training\n",
    "X_text_test = text_scaler.transform(X_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a4b0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.4844845         nan 0.50983882        nan 0.56161404\n",
      "        nan 0.6100755         nan 0.61810105        nan 0.61069539\n",
      "        nan 0.59003459        nan 0.56898571        nan 0.55813576\n",
      "        nan 0.55017537]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter for n_neighbors is {'C': 0.3593813663804626, 'penalty': 'l2'}\n",
      "Scoring 0.6181010502867073\n"
     ]
    }
   ],
   "source": [
    "#Let's begin with a Logisitic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Intialize model\n",
    "log_title = LogisticRegression(solver='newton-cg')\n",
    "#Create a parameter grid over which we can gridsearch\n",
    "param_grid ={'C':np.logspace(-4, 4, 10) , 'penalty':['l1', 'l2']}\n",
    "#Intialize a gridsearch object\n",
    "log_title_gs = GridSearchCV(log_title, param_grid, cv=5, scoring='roc_auc')\n",
    "#Fit the gridsearch object\n",
    "log_title_gs.fit(X_title_train, y_title_train)\n",
    "#Print out best hyperparameters\n",
    "print('Best parameter for n_neighbors is {}'.format(log_title_gs.best_params_))\n",
    "print('Scoring {}'.format(log_title_gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885423bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.60706642        nan 0.6199985         nan 0.62372255\n",
      "        nan 0.62112093        nan 0.61590143        nan 0.61273316\n",
      "        nan 0.61045904        nan 0.60877647        nan 0.60747085\n",
      "        nan 0.60709953]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter for n_neighbors is {'C': 0.005994842503189409, 'penalty': 'l2'}\n",
      "Scoring 0.6237225483991814\n"
     ]
    }
   ],
   "source": [
    "#Intialize model\n",
    "log_text = LogisticRegression(solver='newton-cg')\n",
    "#Intialize a gridsearch object\n",
    "log_text_gs = GridSearchCV(log_text, param_grid, cv=5, scoring='roc_auc')\n",
    "#Fit the gridsearch object\n",
    "log_text_gs.fit(X_text_train, y_text_train)\n",
    "#Print out best hyperparameters\n",
    "print('Best parameter for n_neighbors is {}'.format(log_text_gs.best_params_))\n",
    "print('Scoring {}'.format(log_text_gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5cb6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb80ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get vocab of title_df, exclude first two elements as they are not training features\n",
    "vocab = comment_puller.title_df.columns.tolist()[2:]\n",
    "embedding_matrix = np.zeros((len(vocab) + 1, 100))\n",
    "for i, word in enumerate(vocab):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25877294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 460, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 73, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 177, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 2343, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 625, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 889) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-385b394d535b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                          metrics=['accuracy', 'AUC'])\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#Fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m title_deep_model.fit(X_title_train, y_title_train, epochs=20, verbose=2, \n\u001b[0m\u001b[0;32m     26\u001b[0m                      validation_data=(X_title_test, y_title_test))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 460, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 73, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 177, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 2343, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 625, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 889) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "#Let's try a deep learning model now\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.models import Sequential#Try pytorch\n",
    "#Intialize a model\n",
    "title_deep_model = Sequential()#Try CNN, try sklearn NN mlpclassifier\n",
    "#Add layers\n",
    "title_deep_model.add(Embedding(len(vocab) + 1, 100, weights=[embedding_matrix], \n",
    "                               input_length=X_title_train.shape[1] , trainable=False))\n",
    "title_deep_model.add(Dense(50, activation='relu', input_shape=(X_title_train.shape[1],), \n",
    "                          kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)))\n",
    "title_deep_model.add(Dropout(0.3))\n",
    "title_deep_model.add(Dense(25, activation='relu', \n",
    "                    kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "title_deep_model.add(Dropout(0.4))\n",
    "title_deep_model.add(Dense(5, activation='relu'))\n",
    "title_deep_model.add(Dropout(0.5))\n",
    "title_deep_model.add(Dense(1, activation='sigmoid'))\n",
    "#Compile model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "title_deep_model.compile(optimizer=opt, loss='binary_crossentropy', \n",
    "                         metrics=['accuracy', 'AUC'])\n",
    "#Fit model\n",
    "title_deep_model.fit(X_title_train, y_title_train, epochs=20, verbose=2, \n",
    "                     validation_data=(X_title_test, y_title_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bededb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "95/95 - 1s - loss: 0.4532 - accuracy: 0.8696 - auc: 0.5506 - val_loss: 0.3384 - val_accuracy: 0.8972 - val_auc: 0.6027 - 1s/epoch - 15ms/step\n",
      "Epoch 2/3\n",
      "95/95 - 1s - loss: 0.3351 - accuracy: 0.8818 - auc: 0.7270 - val_loss: 0.3410 - val_accuracy: 0.8972 - val_auc: 0.5996 - 547ms/epoch - 6ms/step\n",
      "Epoch 3/3\n",
      "95/95 - 1s - loss: 0.2113 - accuracy: 0.9241 - auc: 0.9190 - val_loss: 0.5109 - val_accuracy: 0.8935 - val_auc: 0.5914 - 523ms/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x288ae7bd850>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's try with the text data\n",
    "#Intialize a model\n",
    "text_deep_model = Sequential()\n",
    "#Add layers\n",
    "text_deep_model.add(Dense(75, activation='relu', input_shape=(X_text_train.shape[1],)))\n",
    "text_deep_model.add(Dropout(0.3))\n",
    "text_deep_model.add(Dense(30, activation='relu'))#Add dropout layer\n",
    "text_deep_model.add(Dropout(0.3))\n",
    "text_deep_model.add(Dense(5, activation='relu'))\n",
    "text_deep_model.add(Dropout(0.3))\n",
    "text_deep_model.add(Dense(1, activation='sigmoid'))\n",
    "#Compile model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "text_deep_model.compile(optimizer=opt, loss='binary_crossentropy', \n",
    "                         metrics=['accuracy', 'AUC'])\n",
    "#Fit model\n",
    "text_deep_model.fit(X_text_train, y_text_train, epochs=3, verbose=2, \n",
    "                     validation_data=(X_text_test, y_text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39b1ac",
   "metadata": {},
   "source": [
    "These models don't seem to be improving on much. This may be because there isn't enough data, the data doesn't generalize well or some other reason. The BERT training model is a huge model that has already been trained on millions of data examples. Let's see if we can tweak this model and use it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d76ef80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this bert model we need to pass the actual text columns\n",
    "#First let's get title text and labels\n",
    "X_title_bert = comment_puller.post_df['title'].apply(comment_puller._clean_string)\n",
    "y_title_bert = comment_puller.title_df['label_id']\n",
    "#We then split these\n",
    "X_title_train_bert, X_title_test_bert, y_title_train_bert, y_title_test_bert = train_test_split(\n",
    "X_title_bert, y_title_bert, test_size=0.15, random_state=10)\n",
    "#We now do the same for the text body of posts\n",
    "X_text_bert = comment_puller.post_df['post_text'].apply(comment_puller._clean_string)\n",
    "y_text_bert = comment_puller.text_df['label_id']\n",
    "#Split them here\n",
    "X_text_train_bert, X_text_test_bert, y_text_train_bert, y_text_test_bert = train_test_split(\n",
    "X_text_bert, y_text_bert, test_size=0.15, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "184b9d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [brother, inlaw, sammy, lost, home, shortly, d...\n",
       "1       [ill, try, keep, short, impala, doorhttpsiimgu...\n",
       "2       [high, schooler, weekend, job, coffee, shop, c...\n",
       "3       [week, ago, f, family, ordered, chinese, food,...\n",
       "4       [dad, old, fart, love, daughter, piece, ’, str...\n",
       "                              ...                        \n",
       "3559    [screwed, shot, mouth, facebook, said, somethi...\n",
       "3560    [genderfluid, moved, away, transferred, univer...\n",
       "3561    [bf, work, shipping, facility, us, car, get, w...\n",
       "3562    [asshole, wanting, go, funeral, birth, father,...\n",
       "3563    [self, explanatory, started, anti, depressant,...\n",
       "Name: post_text, Length: 3564, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_bert.apply(lambda x: x.split()) #Throw into BertTokenClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6052c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n",
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 2727, validation data size: 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  : 100%|████████████████████████████████████████████████████████| 171/171 [00:51<00:00,  3.31it/s, loss=0.373]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.3729, Val loss: 0.4434, Val accy: 84.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  : 100%|████████████████████████████████████████████████████████| 171/171 [00:51<00:00,  3.30it/s, loss=0.359]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.3591, Val loss: 0.4295, Val accy: 84.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  : 100%|████████████████████████████████████████████████████████| 171/171 [00:51<00:00,  3.30it/s, loss=0.336]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.3360, Val loss: 0.4360, Val accy: 84.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████| 67/67 [00:10<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.3323, Accuracy: 89.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.7196261682243"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import sklearn-like wrapper for BERT model\n",
    "from bert_sklearn import BertClassifier \n",
    "#Intialize model\n",
    "bert_title_model = BertClassifier()\n",
    "#Adjust batch size so it doesn't eat all GPU memory\n",
    "bert_title_model.train_batch_size=16\n",
    "#Tune model\n",
    "bert_title_model.fit(X_title_train_bert, y_title_train_bert)\n",
    "#Score data\n",
    "bert_title_model.score(X_title_test_bert, y_title_test_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "428f6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████████████████████████████████████████████████████████████████| 67/67 [00:11<00:00,  6.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "title_preds = bert_title_model.predict(X_title_test)\n",
    "roc_auc_score(y_title_test_bert, title_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33315d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n",
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 2727, validation data size: 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  : 100%|████████████████████████████████████████████████████████| 171/171 [00:51<00:00,  3.30it/s, loss=0.372]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.3721, Val loss: 0.4449, Val accy: 84.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  : 100%|█████████████████████████████████████████████████████████| 171/171 [00:51<00:00,  3.30it/s, loss=0.36]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:09<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.3603, Val loss: 0.4318, Val accy: 84.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  : 100%|█████████████████████████████████████████████████████████| 171/171 [00:52<00:00,  3.29it/s, loss=0.36]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [00:10<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.3595, Val loss: 0.4319, Val accy: 84.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████| 67/67 [00:10<00:00,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.3318, Accuracy: 89.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.7196261682243"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize model\n",
    "bert_text_model = BertClassifier()\n",
    "#Try throwing in just after cleaning\n",
    "#Then try only from df vocab\n",
    "#Adjust batch size so it doesn't eat all GPU memory\n",
    "bert_text_model.train_batch_size=16\n",
    "#Tune model\n",
    "bert_text_model.fit(X_text_train_bert, y_text_train_bert)\n",
    "#Score data\n",
    "bert_text_model.score(X_text_test_bert, y_text_test_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "876977ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████████████████████████████████████████████████████████████████| 67/67 [00:12<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "text_preds = bert_text_model.predict_proba(X_text_test_bert)#Try another transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "885d1730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5156439393939394"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_text_test_bert, text_preds[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb1772f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertClassifier().epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a025575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open text file to write to \n",
    "f = open('training.txt', 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de554125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For our chat bot, we need to create a text file of post titles, text bodies and comments.\n",
    "#Create an empty string to write to\n",
    "text = \"\"\n",
    "#Make copies of df's to make code easier to read\n",
    "post_df = comment_puller.post_df.copy()\n",
    "comment_df = comment_puller.comment_df.copy()\n",
    "#Get post for specific post_id\n",
    "for post_id in post_df['post_id']:\n",
    "    #Retrieve the post and write the title text and body text\n",
    "    post = post_df[post_df['post_id']==post_id]\n",
    "    text += post['title'].values[0] + '\\n'\n",
    "    text += post['post_text'].values[0] + '\\n'\n",
    "    #Get the top ten comments of post saved in comment_df\n",
    "    comments = comment_df[comment_df['post_id'].isin([post_id])]\n",
    "    #Write them to the text string\n",
    "    for comment in comments['comment']:\n",
    "        text += comment + '/n'\n",
    "    #Write the titles, text bodies and comments to our file\n",
    "    f.write(text)\n",
    "    #Reset text\n",
    "    text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9a237dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ad0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
